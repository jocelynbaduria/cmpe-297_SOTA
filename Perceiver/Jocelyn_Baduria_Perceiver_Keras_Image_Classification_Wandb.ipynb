{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Jocelyn_Baduria_Perceiver_Keras_Image_Classification_Wandb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a4c240a19f194309a933fcfbfc6d2c9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_dd18897391164d4ab3a0bb3a00bf1521",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6d1348ce5d5c490dbe9c145dc454533e",
              "IPY_MODEL_e6ed0f3a895d4693b0fe022f5ea0fddf"
            ]
          }
        },
        "dd18897391164d4ab3a0bb3a00bf1521": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6d1348ce5d5c490dbe9c145dc454533e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_8fbc8fcc384a49828babdc7e848f2805",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d7af7e800d754d6d9965fe6de384ca8b"
          }
        },
        "e6ed0f3a895d4693b0fe022f5ea0fddf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c52fa655a42841a996f75d37eb759e60",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_df5b7cee7de2407ab78b39501f40ffc0"
          }
        },
        "8fbc8fcc384a49828babdc7e848f2805": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d7af7e800d754d6d9965fe6de384ca8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c52fa655a42841a996f75d37eb759e60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "df5b7cee7de2407ab78b39501f40ffc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "47042b3b68034bdeb7bc75daf957da5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_602e93db92194e3cbf13b84f7777590b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_21a0fe5cfe9d44ae8d0af3e0294d1e95",
              "IPY_MODEL_5260f6c5161f49379716f52898f2f335"
            ]
          }
        },
        "602e93db92194e3cbf13b84f7777590b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "21a0fe5cfe9d44ae8d0af3e0294d1e95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_657cdc3e8d734843b7f8f215e70c213e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_62c7f434d7834a628d5b981a7aec9184"
          }
        },
        "5260f6c5161f49379716f52898f2f335": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_93a0057699f1479ea3ce65d51eb7dc21",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fe702940f6d74dd9bba622f53470e471"
          }
        },
        "657cdc3e8d734843b7f8f215e70c213e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "62c7f434d7834a628d5b981a7aec9184": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "93a0057699f1479ea3ce65d51eb7dc21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fe702940f6d74dd9bba622f53470e471": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7LV2AcjhxM1"
      },
      "source": [
        "ReadMe:\n",
        "\n",
        "1. Reproduce the Sample code for understamding of Perceiver Implementation SOTA technology for image classification\n",
        "\n",
        "      Perceiver use data augmentation, Feedforward network, patch creation and encoding of patch layer. To build the Perceiver model you need to implement Cross-attention, Transformer module.\n",
        "\n",
        "2. Run the code using the CIFAR-100 dataset with GPU runtime, 10 epochs. \n",
        "\n",
        "      Results : 1 epoch took 4Hrs and didnt complete the one epoch run.\n",
        "\n",
        "      Epoch 1/10\n",
        "        404/704 [================>.............] - ETA: 4:44:34 4Hrs - loss: 4.4256 - acc: 0.0418 - top5-acc: 0.1474\n",
        "\n",
        "3. Use Weights and Biases to run the code with GPU runtime, 10 epochs.\n",
        "\n",
        "      Hyperparameters use first run : \n",
        "\n",
        "      Image size: 64 X 64 = 4096\n",
        "\n",
        "      Patch size: 2 X 2 = 4 \n",
        "\n",
        "      Patches per image: 1024\n",
        "\n",
        "      Elements per patch (3 channels): 12\n",
        "\n",
        "      Latent array shape: 256 X 256\n",
        "\n",
        "      Data array shape: 1024 X 256\n",
        "\n",
        "    Results : The hyperparameters needed to be optimize to use the GPU compute because of huge allocation of tensor with shape[64,256,8,256]. \n",
        "\n",
        "        ResourceExhaustedError:  OOM when allocating tensor with shape[64,256,8,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
        "\t    [[node perceiver/model_1/multi_head_attention_3/einsum_3/Einsum (defined at <ipython-input-15-05efc0397541>:89) ]]\n",
        "        Hint: If you want to see a list of allocated tensors 4Hrs when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n",
        "        [Op:__inference_train_function_25330]\n",
        "\n",
        "        Function call stack:\n",
        "        train_function\n",
        "4. Change the hyperparameters to a lower dimension \n",
        "\n",
        "      Image size: 32 X 32 = 1024\n",
        "\n",
        "      Patch size: 4 X 4 = 16 \n",
        "      \n",
        "      Patches per image: 64\n",
        "      \n",
        "      Elements per patch (3 channels): 48\n",
        "      \n",
        "      Latent array shape: 64 X 64\n",
        "      \n",
        "      Data array shape: 64 X 64\n",
        "\n",
        "      Results : \n",
        "      The model achieved Test accuracy: 55.79%\n",
        "\n",
        "      Test top 5 accuracy: 95.1% running only for 10 epochs.\n",
        "\n",
        "Conclusion : Smaller data array parameters gives higher accuracy compared with bigger data array parameters. \n",
        "\n",
        "Reference : \n",
        "\n",
        "https://keras.io/examples/vision/perceiver_image_classification/\n",
        "\n",
        "https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/perceiver_image_classification.ipynb\n",
        "\n",
        "https://github.com/wandb/client/archive/feature/code-save.zip\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9fFph5V8GZE",
        "outputId": "6dda4ea0-6a04-4723-f977-a080d42ccaab"
      },
      "source": [
        "!pip install -U tensorflow-addons"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.14.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrsGD5aHmEXl"
      },
      "source": [
        "#importing the libraries\n",
        "import os \n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "import PIL\n",
        "import PIL.Image\n",
        "\n",
        "# Imports module\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "\n",
        "from imutils import paths\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Random seed fixation\n",
        "tf.random.set_seed(666)\n",
        "np.random.seed(666)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypBwPbMxBtwM"
      },
      "source": [
        "## Wandb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85bHV2IxBs7z",
        "outputId": "2ed40479-0686-4f44-865a-37bcc91a8eda"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Oct  9 22:45:42 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   73C    P0    74W / 149W |   1196MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXDymJOyw99L",
        "outputId": "3ad68faf-ac7b-4f9a-ae2e-4dbcf12d69de"
      },
      "source": [
        "# Install wandb for experiment tracking\n",
        "!pip install --upgrade https://github.com/wandb/client/archive/feature/code-save.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting https://github.com/wandb/client/archive/feature/code-save.zip\n",
            "  Downloading https://github.com/wandb/client/archive/feature/code-save.zip\n",
            "\u001b[K     - 10.4 MB 550 kB/s\n",
            "\u001b[?25hRequirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.8.36) (7.1.2)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting gql==0.2.0\n",
            "  Downloading gql-0.2.0.tar.gz (18 kB)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.8.36) (7.352.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb==0.8.36) (2.8.2)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.8.36) (2.23.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.8.36) (1.15.0)\n",
            "Collecting watchdog>=0.8.3\n",
            "  Downloading watchdog-2.1.6-py3-none-manylinux2014_x86_64.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from wandb==0.8.36) (3.13)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.8.36) (5.4.8)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "  Downloading sentry_sdk-1.4.3-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 21.7 MB/s \n",
            "\u001b[?25hCollecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.6 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n",
            "Collecting graphql-core<2,>=0.5.0\n",
            "  Downloading graphql-core-1.1.tar.gz (70 kB)\n",
            "\u001b[K     |████████████████████████████████| 70 kB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from gql==0.2.0->wandb==0.8.36) (2.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb==0.8.36) (3.7.4.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->wandb==0.8.36) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->wandb==0.8.36) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->wandb==0.8.36) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->wandb==0.8.36) (2.10)\n",
            "Building wheels for collected packages: wandb, gql, graphql-core, subprocess32\n",
            "  Building wheel for wandb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wandb: filename=wandb-0.8.36-py2.py3-none-any.whl size=1385054 sha256=40bc1b9f3a0d0d3f1d151d66e684129b23d3acb97735e49d126735aa5466b9ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/1e/c2/b01dd11de70589ef2c6305c9b94da5bdfa22fd99b08da54fcf\n",
            "  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gql: filename=gql-0.2.0-py3-none-any.whl size=7639 sha256=3f478df340877313b11214234631ad06da4caca992fef734e3b62462fbd78dfc\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/9a/56/5456fd32264a8fc53eefcb2f74e24e99a7ef4eb40a9af5c905\n",
            "  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for graphql-core: filename=graphql_core-1.1-py3-none-any.whl size=104649 sha256=620e217d6057a9ea1f4f0cba2418962ac9ba03ab11e7eddc89d23fff7e78f6a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/fd/8c/a20dd591c1a554070cc33fb58042867e6ac1c85395abe2e57a\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=a021235f279cbb63d5d9952ec995bd10dbbe28febeac2f7a214fc66e5b2c0ecd\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "Successfully built wandb gql graphql-core subprocess32\n",
            "Installing collected packages: smmap, graphql-core, gitdb, watchdog, subprocess32, shortuuid, sentry-sdk, gql, GitPython, docker-pycreds, configparser, wandb\n",
            "Successfully installed GitPython-3.1.24 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 gql-0.2.0 graphql-core-1.1 sentry-sdk-1.4.3 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 wandb-0.8.36 watchdog-2.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLFj9pC2Im6Z",
        "outputId": "6d6b2cd3-f168-4ab0-9f73-c4e086425b1d"
      },
      "source": [
        "!pip install wandb --upgrade"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.4)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.1)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.24)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.0.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.4.3)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: yaspin>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.7)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ycf_U11gxI8I",
        "outputId": "4e3e704c-a8a6-4961-91e2-fece05b16de9"
      },
      "source": [
        "# Other imports\n",
        "import wandb\n",
        "wandb.login()\n",
        "from wandb.keras import WandbCallback\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmIFZ2R8CBjg"
      },
      "source": [
        "## Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTqSvb_5DnVv",
        "outputId": "6e6c4ad8-bab7-404d-c8b6-f7763d1d5955"
      },
      "source": [
        "num_classes = 100\n",
        "input_shape = (32, 32, 3)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
            "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UzyAvSnTMOW",
        "outputId": "3065268b-0101-4904-d7b1-7707801c70ac"
      },
      "source": [
        "# Use CIFAR-10 not CIFAR-100\n",
        "num_classes = 100\n",
        "input_shape = (32, 32, 3)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 2s 0us/step\n",
            "169017344/169001437 [==============================] - 2s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
            "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GU0QG6nn95CF"
      },
      "source": [
        "## Configure the hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu5xWNfYAMP5"
      },
      "source": [
        "First hyperparameter tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVxTlhyliCla",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cb98b90-442b-49a5-94a1-e5ef4ef8a26f"
      },
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 64\n",
        "num_epochs = 50\n",
        "dropout_rate = 0.2\n",
        "image_size = 64  # We'll resize input images to this size.\n",
        "patch_size = 2  # Size of the patches to be extract from the input images.\n",
        "num_patches = (image_size // patch_size) ** 2  # Size of the data array.\n",
        "latent_dim = 256  # Size of the latent array.\n",
        "projection_dim = 256  # Embedding size of each element in the data and latent arrays.\n",
        "num_heads = 8  # Number of Transformer heads.\n",
        "ffn_units = [\n",
        "    projection_dim,\n",
        "    projection_dim,\n",
        "]  # Size of the Transformer Feedforward network.\n",
        "num_transformer_blocks = 4\n",
        "num_iterations = 2  # Repetitions of the cross-attention and Transformer modules.\n",
        "classifier_units = [\n",
        "    projection_dim,\n",
        "    num_classes,\n",
        "]  # Size of the Feedforward network of the final classifier.\n",
        "\n",
        "print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n",
        "print(f\"Patches per image: {num_patches}\")\n",
        "print(f\"Elements per patch (3 channels): {(patch_size ** 2) * 3}\")\n",
        "print(f\"Latent array shape: {latent_dim} X {projection_dim}\")\n",
        "print(f\"Data array shape: {num_patches} X {projection_dim}\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image size: 64 X 64 = 4096\n",
            "Patch size: 2 X 2 = 4 \n",
            "Patches per image: 1024\n",
            "Elements per patch (3 channels): 12\n",
            "Latent array shape: 256 X 256\n",
            "Data array shape: 1024 X 256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OziBrTLEARWz"
      },
      "source": [
        "Second hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6_5vkKx95CG",
        "outputId": "dd2836da-2c42-49ca-bd5c-85ca0f00e092"
      },
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 64\n",
        "# num_epochs = 50\n",
        "num_epochs = 10\n",
        "dropout_rate = 0.2\n",
        "# image_size = 64  # We'll resize input images to this size.\n",
        "image_size = 32  # We'll resize input images to this size.\n",
        "# patch_size = 2  # Size of the patches to be extract from the input images.\n",
        "patch_size = 4  # Size of the patches to be extract from the input images.\n",
        "num_patches = (image_size // patch_size) ** 2  # Size of the data array.\n",
        "# latent_dim = 256  # Size of the latent array.\n",
        "latent_dim = 64  # Size of the latent array.\n",
        "# projection_dim = 256  # Embedding size of each element in the data and latent arrays.\n",
        "projection_dim = 64  # Embedding size of each element in the data and latent arrays.\n",
        "# num_heads = 8  # Number of Transformer heads.\n",
        "num_heads = 2  # Number of Transformer heads.\n",
        "ffn_units = [\n",
        "    projection_dim,\n",
        "    projection_dim,\n",
        "]  # Size of the Transformer Feedforward network.\n",
        "num_transformer_blocks = 4\n",
        "num_iterations = 2  # Repetitions of the cross-attention and Transformer modules.\n",
        "classifier_units = [\n",
        "    projection_dim,\n",
        "    num_classes,\n",
        "]  # Size of the Feedforward network of the final classifier.\n",
        "\n",
        "print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n",
        "print(f\"Patches per image: {num_patches}\")\n",
        "print(f\"Elements per patch (3 channels): {(patch_size ** 2) * 3}\")\n",
        "print(f\"Latent array shape: {latent_dim} X {projection_dim}\")\n",
        "print(f\"Data array shape: {num_patches} X {projection_dim}\")"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image size: 32 X 32 = 1024\n",
            "Patch size: 4 X 4 = 16 \n",
            "Patches per image: 64\n",
            "Elements per patch (3 channels): 48\n",
            "Latent array shape: 64 X 64\n",
            "Data array shape: 64 X 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB2MNI7F95CH"
      },
      "source": [
        "Note that, in order to use each pixel as an individual input in the data array,\n",
        "set `patch_size` to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTWNUHvR7s-j"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-92ur9X95CI"
      },
      "source": [
        "## Use data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qM000pLD95CI"
      },
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(image_size, image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(x_train)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VckQJqO48WD0"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opUQzz9N95CJ"
      },
      "source": [
        "## Implement Feedforward network (FFN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQtwhkpI95CJ"
      },
      "source": [
        "\n",
        "def create_ffn(hidden_units, dropout_rate):\n",
        "    ffn_layers = []\n",
        "    for units in hidden_units[:-1]:\n",
        "        ffn_layers.append(layers.Dense(units, activation=tf.nn.gelu))\n",
        "\n",
        "    ffn_layers.append(layers.Dense(units=hidden_units[-1]))\n",
        "    ffn_layers.append(layers.Dropout(dropout_rate))\n",
        "\n",
        "    ffn = keras.Sequential(ffn_layers)\n",
        "    return ffn\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jh0vUKc58gbm"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYm5eObA95CK"
      },
      "source": [
        "## Implement patch creation as a layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lG8W_ssB95CL"
      },
      "source": [
        "\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_avyf9IT8lz1"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al2Ne6xJ95CM"
      },
      "source": [
        "## Implement the patch encoding layer\n",
        "\n",
        "The `PatchEncoder` layer will linearly transform a patch by projecting it into\n",
        "a vector of size `latent_dim`. In addition, it adds a learnable position embedding\n",
        "to the projected vector.\n",
        "\n",
        "Note that the orginal Perceiver paper uses the Fourier feature positional encodings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue-AJove95CN"
      },
      "source": [
        "\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patches):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patches) + self.position_embedding(positions)\n",
        "        return encoded\n"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR1a170f8qRM"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dke2hC9N95CO"
      },
      "source": [
        "## Build the Perceiver model\n",
        "\n",
        "The Perceiver consists of two modules: a cross-attention\n",
        "module and a standard Transformer with self-attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUj3LQFc95CO"
      },
      "source": [
        "### Cross-attention module\n",
        "\n",
        "The cross-attention expects a `(latent_dim, projection_dim)` latent array,\n",
        "and the `(data_dim,  projection_dim)` data array as inputs,\n",
        "to produce a `(latent_dim, projection_dim)` latent array as an output.\n",
        "To apply cross-attention, the `query` vectors are generated from the latent array,\n",
        "while the `key` and `value` vectors are generated from the encoded image.\n",
        "\n",
        "Note that the data array in this example is the image,\n",
        "where the `data_dim` is set to the `num_patches`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXUTQUmP95CP"
      },
      "source": [
        "\n",
        "def create_cross_attention_module(\n",
        "    latent_dim, data_dim, projection_dim, ffn_units, dropout_rate\n",
        "):\n",
        "\n",
        "    inputs = {\n",
        "        # Recieve the latent array as an input of shape [1, latent_dim, projection_dim].\n",
        "        \"latent_array\": layers.Input(shape=(latent_dim, projection_dim)),\n",
        "        # Recieve the data_array (encoded image) as an input of shape [batch_size, data_dim, projection_dim].\n",
        "        \"data_array\": layers.Input(shape=(data_dim, projection_dim)),\n",
        "    }\n",
        "\n",
        "    # Apply layer norm to the inputs\n",
        "    latent_array = layers.LayerNormalization(epsilon=1e-6)(inputs[\"latent_array\"])\n",
        "    data_array = layers.LayerNormalization(epsilon=1e-6)(inputs[\"data_array\"])\n",
        "\n",
        "    # Create query tensor: [1, latent_dim, projection_dim].\n",
        "    query = layers.Dense(units=projection_dim)(latent_array)\n",
        "    # Create key tensor: [batch_size, data_dim, projection_dim].\n",
        "    key = layers.Dense(units=projection_dim)(data_array)\n",
        "    # Create value tensor: [batch_size, data_dim, projection_dim].\n",
        "    value = layers.Dense(units=projection_dim)(data_array)\n",
        "\n",
        "    # Generate cross-attention outputs: [batch_size, latent_dim, projection_dim].\n",
        "    attention_output = layers.Attention(use_scale=True, dropout=0.1)(\n",
        "        [query, key, value], return_attention_scores=False\n",
        "    )\n",
        "    # Skip connection 1.\n",
        "    attention_output = layers.Add()([attention_output, latent_array])\n",
        "\n",
        "    # Apply layer norm.\n",
        "    attention_output = layers.LayerNormalization(epsilon=1e-6)(attention_output)\n",
        "    # Apply Feedforward network.\n",
        "    ffn = create_ffn(hidden_units=ffn_units, dropout_rate=dropout_rate)\n",
        "    outputs = ffn(attention_output)\n",
        "    # Skip connection 2.\n",
        "    outputs = layers.Add()([outputs, attention_output])\n",
        "\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MAQoP1N95CQ"
      },
      "source": [
        "### Transformer module\n",
        "\n",
        "The Transformer expects the output latent vector from the cross-attention module\n",
        "as an input, applies multi-head self-attention to its `latent_dim` elements,\n",
        "followed by feedforward network, to produce another `(latent_dim, projection_dim)` latent array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8uQ0evQ95CR"
      },
      "source": [
        "\n",
        "def create_transformer_module(\n",
        "    latent_dim,\n",
        "    projection_dim,\n",
        "    num_heads,\n",
        "    num_transformer_blocks,\n",
        "    ffn_units,\n",
        "    dropout_rate,\n",
        "):\n",
        "\n",
        "    # input_shape: [1, latent_dim, projection_dim]\n",
        "    inputs = layers.Input(shape=(latent_dim, projection_dim))\n",
        "\n",
        "    x0 = inputs\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        # Apply layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(x0)\n",
        "        # Create a multi-head self-attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, x0])\n",
        "        # Apply layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # Apply Feedforward network.\n",
        "        ffn = create_ffn(hidden_units=ffn_units, dropout_rate=dropout_rate)\n",
        "        x3 = ffn(x3)\n",
        "        # Skip connection 2.\n",
        "        x0 = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=x0)\n",
        "    return model\n"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW-uvQs795CS"
      },
      "source": [
        "### Perceiver model\n",
        "\n",
        "The Perceiver model repeats the cross-attention and Transformer modules\n",
        "`num_iterations` times—with shared weights and skip connections—to allow\n",
        "the latent array to iteratively extract information from the input image as it is needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFq4rcpp95CS"
      },
      "source": [
        "\n",
        "class Perceiver(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        patch_size,\n",
        "        data_dim,\n",
        "        latent_dim,\n",
        "        projection_dim,\n",
        "        num_heads,\n",
        "        num_transformer_blocks,\n",
        "        ffn_units,\n",
        "        dropout_rate,\n",
        "        num_iterations,\n",
        "        classifier_units,\n",
        "    ):\n",
        "        super(Perceiver, self).__init__()\n",
        "\n",
        "        self.latent_dim = latent_dim\n",
        "        self.data_dim = data_dim\n",
        "        self.patch_size = patch_size\n",
        "        self.projection_dim = projection_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.num_transformer_blocks = num_transformer_blocks\n",
        "        self.ffn_units = ffn_units\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.classifier_units = classifier_units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Create latent array.\n",
        "        self.latent_array = self.add_weight(\n",
        "            shape=(self.latent_dim, self.projection_dim),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "        # Create patching module.\n",
        "        self.patcher = Patches(self.patch_size)\n",
        "\n",
        "        # Create patch encoder.\n",
        "        self.patch_encoder = PatchEncoder(self.data_dim, self.projection_dim)\n",
        "\n",
        "        # Create cross-attenion module.\n",
        "        self.cross_attention = create_cross_attention_module(\n",
        "            self.latent_dim,\n",
        "            self.data_dim,\n",
        "            self.projection_dim,\n",
        "            self.ffn_units,\n",
        "            self.dropout_rate,\n",
        "        )\n",
        "\n",
        "        # Create Transformer module.\n",
        "        self.transformer = create_transformer_module(\n",
        "            self.latent_dim,\n",
        "            self.projection_dim,\n",
        "            self.num_heads,\n",
        "            self.num_transformer_blocks,\n",
        "            self.ffn_units,\n",
        "            self.dropout_rate,\n",
        "        )\n",
        "\n",
        "        # Create global average pooling layer.\n",
        "        self.global_average_pooling = layers.GlobalAveragePooling1D()\n",
        "\n",
        "        # Create a classification head.\n",
        "        self.classification_head = create_ffn(\n",
        "            hidden_units=self.classifier_units, dropout_rate=self.dropout_rate\n",
        "        )\n",
        "\n",
        "        super(Perceiver, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Augment data.\n",
        "        augmented = data_augmentation(inputs)\n",
        "        # Create patches.\n",
        "        patches = self.patcher(augmented)\n",
        "        # Encode patches.\n",
        "        encoded_patches = self.patch_encoder(patches)\n",
        "        # Prepare cross-attention inputs.\n",
        "        cross_attention_inputs = {\n",
        "            \"latent_array\": tf.expand_dims(self.latent_array, 0),\n",
        "            \"data_array\": encoded_patches,\n",
        "        }\n",
        "        # Apply the cross-attention and the Transformer modules iteratively.\n",
        "        for _ in range(self.num_iterations):\n",
        "            # Apply cross-attention from the latent array to the data array.\n",
        "            latent_array = self.cross_attention(cross_attention_inputs)\n",
        "            # Apply self-attention Transformer to the latent array.\n",
        "            latent_array = self.transformer(latent_array)\n",
        "            # Set the latent array of the next iteration.\n",
        "            cross_attention_inputs[\"latent_array\"] = latent_array\n",
        "\n",
        "        # Apply global average pooling to generate a [batch_size, projection_dim] repesentation tensor.\n",
        "        representation = self.global_average_pooling(latent_array)\n",
        "        # Generate logits.\n",
        "        logits = self.classification_head(representation)\n",
        "        return logits\n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuU9nW7E86Hu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzrblFLF95CT"
      },
      "source": [
        "## Compile, train, and evaluate the mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6j5I5-fJ95CV"
      },
      "source": [
        "\n",
        "def run_experiment(model):\n",
        "\n",
        "    # Create LAMB optimizer with weight decay.\n",
        "    optimizer = tfa.optimizers.LAMB(\n",
        "        learning_rate=learning_rate, weight_decay_rate=weight_decay,\n",
        "    )\n",
        "\n",
        "    # Compile the model.\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # Create a learning rate scheduler callback.\n",
        "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\", factor=0.2, patience=3\n",
        "    )\n",
        "\n",
        "    # Create an early stopping callback.\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\", patience=15, restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    # Fit the model.\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[early_stopping, reduce_lr],\n",
        "    )\n",
        "\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    # Return history to plot learning curves.\n",
        "    return history\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNpUucr7Fc5V"
      },
      "source": [
        "First run using GPU Colab \n",
        "It took 4hrs for one epoch and the colab crashed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wYnO6O9Fhx6",
        "outputId": "686b94f2-fdce-41b6-d6f8-7c5af400aaae"
      },
      "source": [
        "perceiver_classifier = Perceiver(\n",
        "    patch_size,\n",
        "    num_patches,\n",
        "    latent_dim,\n",
        "    projection_dim,\n",
        "    num_heads,\n",
        "    num_transformer_blocks,\n",
        "    ffn_units,\n",
        "    dropout_rate,\n",
        "    num_iterations,\n",
        "    classifier_units,\n",
        ")\n",
        "\n",
        "\n",
        "history = run_experiment(perceiver_classifier)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "404/704 [================>.............] - ETA: 4:44:34 - loss: 4.4256 - acc: 0.0418 - top5-acc: 0.1474"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2EPMny-Ft8B"
      },
      "source": [
        "Use Wandb (Weight and Bias) GPU runtime and 50(first run) and 10(second run) epochs experiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvcagAkpBWaG"
      },
      "source": [
        "## Run this for Wandb\n",
        "perceiver_classifier = Perceiver(\n",
        "    patch_size,\n",
        "    num_patches,\n",
        "    latent_dim,\n",
        "    projection_dim,\n",
        "    num_heads,\n",
        "    num_transformer_blocks,\n",
        "    ffn_units,\n",
        "    dropout_rate,\n",
        "    num_iterations,\n",
        "    classifier_units,\n",
        ")"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0-miOuma_9W"
      },
      "source": [
        "def plot_training(H):\n",
        "\twith plt.xkcd():\n",
        "\t\tplt.plot(H.history[\"val_loss\"], label=\"val_loss\")\n",
        "\t\tplt.plot(H.history[\"acc\"], label=\"test_acc\")\n",
        "\t\tplt.title(\"Training Loss and Accuracy\")\n",
        "\t\tplt.xlabel(\"Epoch #\")\n",
        "\t\tplt.ylabel(\"Loss/Accuracy\")\n",
        "\t\tplt.legend(loc=\"lower left\")\n",
        "\t\tplt.show()"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9OIlKrS95CW"
      },
      "source": [
        "Note that training the perceiver model with the current settings on a V100 GPUs takes\n",
        "around 200 seconds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aampzZsV95CX"
      },
      "source": [
        "## DO NOT RUN\n",
        "# perceiver_classifier = Perceiver(\n",
        "#     patch_size,\n",
        "#     num_patches,\n",
        "#     latent_dim,\n",
        "#     projection_dim,\n",
        "#     num_heads,\n",
        "#     num_transformer_blocks,\n",
        "#     ffn_units,\n",
        "#     dropout_rate,\n",
        "#     num_iterations,\n",
        "#     classifier_units,\n",
        "# )\n",
        "\n",
        "\n",
        "# history = run_experiment(perceiver_classifier)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786,
          "referenced_widgets": [
            "a4c240a19f194309a933fcfbfc6d2c9f",
            "dd18897391164d4ab3a0bb3a00bf1521",
            "6d1348ce5d5c490dbe9c145dc454533e",
            "e6ed0f3a895d4693b0fe022f5ea0fddf",
            "8fbc8fcc384a49828babdc7e848f2805",
            "d7af7e800d754d6d9965fe6de384ca8b",
            "c52fa655a42841a996f75d37eb759e60",
            "df5b7cee7de2407ab78b39501f40ffc0"
          ]
        },
        "id": "b4eoquPaAN5o",
        "outputId": "6d1929b5-c64f-4d72-f319-f7989072f680"
      },
      "source": [
        "# First run\n",
        "# Train the supervised model with full data\n",
        "wandb.init(project=\"perceiver0\", id=\"self-supervised-learning-training0\")\n",
        "\n",
        "model = run_experiment(perceiver_classifier)\n",
        "\n",
        "plot_training(model)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Finishing last run (ID:self-supervised-learning-training06) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 1506... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4c240a19f194309a933fcfbfc6d2c9f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
              "</div><div class=\"wandb-col\">\n",
              "</div></div>\n",
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
              "<br/>Synced <strong style=\"color:#cdcd00\">self-supervised-learning-training06</strong>: <a href=\"https://wandb.ai/jocelynbb/perceiver06/runs/self-supervised-learning-training06\" target=\"_blank\">https://wandb.ai/jocelynbb/perceiver06/runs/self-supervised-learning-training06</a><br/>\n",
              "Find logs at: <code>./wandb/run-20211009_223008-self-supervised-learning-training06/logs</code><br/>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Successfully finished last run (ID:self-supervised-learning-training06). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/jocelynbb/perceiver0/runs/self-supervised-learning-training0\" target=\"_blank\">self-supervised-learning-training0</a></strong> to <a href=\"https://wandb.ai/jocelynbb/perceiver0\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-75e64421ef21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"perceiver0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"self-supervised-learning-training0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperceiver_classifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplot_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-203edd3b2275>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_lr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     )\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/integration/keras/keras.py\u001b[0m in \u001b[0;36mnew_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcbk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcbks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mset_wandb_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mold_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0mtraining_arrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_fit_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[64,8,256,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node gradient_tape/perceiver_5/model_1/multi_head_attention_3/einsum_3/Einsum (defined at /usr/local/lib/python3.7/dist-packages/wandb/integration/keras/keras.py:150) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_292486]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node gradient_tape/perceiver_5/model_1/multi_head_attention_3/einsum_3/Einsum:\n perceiver_5/model_1/multi_head_attention_3/value/add_1 (defined at <ipython-input-14-05efc0397541>:89)\n\nFunction call stack:\ntrain_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBWxIBYCW34u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870,
          "referenced_widgets": [
            "47042b3b68034bdeb7bc75daf957da5b",
            "602e93db92194e3cbf13b84f7777590b",
            "21a0fe5cfe9d44ae8d0af3e0294d1e95",
            "5260f6c5161f49379716f52898f2f335",
            "657cdc3e8d734843b7f8f215e70c213e",
            "62c7f434d7834a628d5b981a7aec9184",
            "93a0057699f1479ea3ce65d51eb7dc21",
            "fe702940f6d74dd9bba622f53470e471"
          ]
        },
        "id": "4p5PgvBpaT-Y",
        "outputId": "e4d6b619-7a10-4ec6-a681-cad056fd2fec"
      },
      "source": [
        "# Second run\n",
        "# Train the supervised model with full data\n",
        "wandb.init(project=\"perceiver06\", id=\"self-supervised-learning-training06\")\n",
        "\n",
        "model = run_experiment(perceiver_classifier)\n",
        "\n",
        "plot_training(model)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Finishing last run (ID:self-supervised-learning-training0) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 1746... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47042b3b68034bdeb7bc75daf957da5b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
              "</div><div class=\"wandb-col\">\n",
              "</div></div>\n",
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
              "<br/>Synced <strong style=\"color:#cdcd00\">self-supervised-learning-training0</strong>: <a href=\"https://wandb.ai/jocelynbb/perceiver0/runs/self-supervised-learning-training0\" target=\"_blank\">https://wandb.ai/jocelynbb/perceiver0/runs/self-supervised-learning-training0</a><br/>\n",
              "Find logs at: <code>./wandb/run-20211009_224732-self-supervised-learning-training0/logs</code><br/>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Successfully finished last run (ID:self-supervised-learning-training0). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/jocelynbb/perceiver06/runs/self-supervised-learning-training06\" target=\"_blank\">self-supervised-learning-training06</a></strong> to <a href=\"https://wandb.ai/jocelynbb/perceiver06\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "704/704 [==============================] - 103s 110ms/step - loss: 2.5802 - acc: 0.2483 - top5-acc: 0.6976 - val_loss: 1.8219 - val_acc: 0.3248 - val_top5-acc: 0.8384\n",
            "Epoch 2/10\n",
            "704/704 [==============================] - 75s 107ms/step - loss: 2.2817 - acc: 0.3330 - top5-acc: 0.7816 - val_loss: 1.6329 - val_acc: 0.3978 - val_top5-acc: 0.8928\n",
            "Epoch 3/10\n",
            "704/704 [==============================] - 75s 107ms/step - loss: 2.1877 - acc: 0.3674 - top5-acc: 0.8091 - val_loss: 1.5614 - val_acc: 0.4266 - val_top5-acc: 0.9030\n",
            "Epoch 4/10\n",
            "704/704 [==============================] - 75s 107ms/step - loss: 2.1058 - acc: 0.3935 - top5-acc: 0.8331 - val_loss: 1.4439 - val_acc: 0.4694 - val_top5-acc: 0.9272\n",
            "Epoch 5/10\n",
            "704/704 [==============================] - 75s 107ms/step - loss: 2.0428 - acc: 0.4184 - top5-acc: 0.8507 - val_loss: 1.4021 - val_acc: 0.4898 - val_top5-acc: 0.9318\n",
            "Epoch 6/10\n",
            "704/704 [==============================] - 75s 107ms/step - loss: 1.9881 - acc: 0.4363 - top5-acc: 0.8622 - val_loss: 1.3452 - val_acc: 0.5164 - val_top5-acc: 0.9428\n",
            "Epoch 7/10\n",
            "704/704 [==============================] - 76s 108ms/step - loss: 1.9457 - acc: 0.4484 - top5-acc: 0.8752 - val_loss: 1.3336 - val_acc: 0.5082 - val_top5-acc: 0.9430\n",
            "Epoch 8/10\n",
            "704/704 [==============================] - 75s 106ms/step - loss: 1.9044 - acc: 0.4617 - top5-acc: 0.8810 - val_loss: 1.3064 - val_acc: 0.5138 - val_top5-acc: 0.9492\n",
            "Epoch 9/10\n",
            "704/704 [==============================] - 75s 107ms/step - loss: 1.8864 - acc: 0.4699 - top5-acc: 0.8858 - val_loss: 1.3094 - val_acc: 0.5280 - val_top5-acc: 0.9482\n",
            "Epoch 10/10\n",
            "704/704 [==============================] - 75s 107ms/step - loss: 1.8558 - acc: 0.4794 - top5-acc: 0.8935 - val_loss: 1.2209 - val_acc: 0.5556 - val_top5-acc: 0.9548\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 1.2430 - acc: 0.5505 - top5-acc: 0.9516\n",
            "Test accuracy: 55.05%\n",
            "Test top 5 accuracy: 95.16%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g6WpkBb95CX"
      },
      "source": [
        "After 10 epochs, the Perceiver model achieves around 55% accuracy and 95% top-5 accuracy on the test data.\n",
        "\n",
        "As mentioned in the ablations of the [Perceiver](https://arxiv.org/abs/2103.03206) paper,\n",
        "you can obtain better results by increasing the latent array size,\n",
        "increasing the (projection) dimensions of the latent array and data array elements, however my evaluations shows lower dimension projection shows better accuracy.\n",
        "\n",
        "I decrease the latent array, data array shape but increase the elements per patch improve the image accuracy. \n",
        "\n",
        "Using high dimensional array requires a higher compute and the GPU can't process it.\n",
        "\n",
        "Before :\n",
        "\n",
        "Image size: 64 X 64 = 4096\n",
        "\n",
        "Patch size: 2 X 2 = 4 \n",
        "\n",
        "Patches per image: 1024\n",
        "\n",
        "Elements per patch (3 channels): 12\n",
        "\n",
        "Latent array shape: 256 X 256\n",
        "\n",
        "Data array shape: 1024 X 256\n",
        "\n",
        "After :\n",
        "\n",
        "Image size: 32 X 32 = 1024\n",
        "\n",
        "Patch size: 4 X 4 = 16 \n",
        "\n",
        "Patches per image: 64\n",
        "\n",
        "Elements per patch (3 channels): 48\n",
        "\n",
        "Latent array shape: 64 X 64\n",
        "\n",
        "Data array shape: 64 X 64\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_CDxlShVZ5u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}